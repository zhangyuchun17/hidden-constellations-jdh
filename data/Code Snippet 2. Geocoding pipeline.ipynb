{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code Snippet 2. Geocoding pipeline"
      ],
      "metadata": {
        "id": "DJASXj6FTwt8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncWUYn6EPoAl",
        "outputId": "c33608c8-2108-4092-f651-a636743abeae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: harmonized_geocoded.csv (rows=542)\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import io\n",
        "import csv\n",
        "import re\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Inputs / outputs\n",
        "# ---------------------------\n",
        "IN_HARMONIZED = \"/content/harmonized_core.csv\"\n",
        "S3_PATH = \"/content/S3_Gwendolyn Stegall data.xlsx\"\n",
        "OUT_GEOCODED = \"harmonized_geocoded.csv\"\n",
        "\n",
        "CENSUS_BATCH_URL = \"https://geocoding.geo.census.gov/geocoder/locations/addressbatch\"\n",
        "CENSUS_BENCHMARK = \"Public_AR_Current\"\n",
        "CENSUS_VINTAGE = \"Current_Current\"\n",
        "\n",
        "STATE_DEFAULT = \"NY\"\n",
        "ZIP_RE = re.compile(r\"\\b(\\d{5})(?:-\\d{4})?\\b\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Address cleaning / classification\n",
        "# ---------------------------\n",
        "def norm_ws(x: Optional[str]) -> Optional[str]:\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s if s else None\n",
        "\n",
        "\n",
        "def strip_parentheses(s: str) -> str:\n",
        "    return re.sub(r\"\\s*\\([^)]*\\)\\s*\", \" \", s).strip()\n",
        "\n",
        "\n",
        "def extract_zip(addr: Optional[str]) -> Optional[str]:\n",
        "    a = norm_ws(addr)\n",
        "    if not a:\n",
        "        return None\n",
        "    m = ZIP_RE.search(a)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "\n",
        "def classify_address_kind(addr: Optional[str]) -> str:\n",
        "    a = norm_ws(addr)\n",
        "    if not a:\n",
        "        return \"missing\"\n",
        "\n",
        "    a = strip_parentheses(a).lower()\n",
        "\n",
        "    if re.search(r\"\\bbetween\\b\", a):\n",
        "        return \"between_phrase\"\n",
        "    if \" and \" in a or \" & \" in a:\n",
        "        return \"intersection\"\n",
        "    if \";\" in a or \" or \" in a or \" later \" in a or \" and later \" in a:\n",
        "        return \"multi_address\"\n",
        "    if re.match(r\"^\\d\", a):\n",
        "        return \"street_address\"\n",
        "\n",
        "    return \"descriptive\"\n",
        "\n",
        "\n",
        "def clean_street_for_geocode(addr: Optional[str]) -> Optional[str]:\n",
        "    a = norm_ws(addr)\n",
        "    if not a:\n",
        "        return None\n",
        "\n",
        "    a = strip_parentheses(a)\n",
        "    a = a.replace(\"‐\", \"-\").replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
        "    a = re.sub(r\"\\s+\", \" \", a).strip()\n",
        "\n",
        "    a_lower = a.lower()\n",
        "\n",
        "    if re.search(r\"\\bbetween\\b\", a_lower):\n",
        "        return None\n",
        "    if \" and \" in a_lower or \" & \" in a_lower:\n",
        "        return None\n",
        "    if \";\" in a_lower or \" or \" in a_lower or \" later \" in a_lower or \" and later \" in a_lower:\n",
        "        return None\n",
        "\n",
        "    a = re.sub(\n",
        "        r\",\\s*(New York|NYC|Manhattan|Brooklyn|Queens|Bronx|Staten Island)\\b.*$\",\n",
        "        \"\",\n",
        "        a,\n",
        "        flags=re.I,\n",
        "    ).strip()\n",
        "    a = re.sub(r\",\\s*NY\\b.*$\", \"\", a, flags=re.I).strip()\n",
        "\n",
        "    return a if a else None\n",
        "\n",
        "\n",
        "def borough_to_city(borough: Optional[str]) -> str:\n",
        "    if not borough:\n",
        "        return \"New York\"\n",
        "    b = borough.strip().lower()\n",
        "    if b.startswith(\"manhattan\"):\n",
        "        return \"New York\"\n",
        "    if b.startswith(\"brooklyn\"):\n",
        "        return \"Brooklyn\"\n",
        "    if b.startswith(\"queens\"):\n",
        "        return \"Queens\"\n",
        "    if b.startswith(\"bronx\"):\n",
        "        return \"Bronx\"\n",
        "    if \"staten\" in b:\n",
        "        return \"Staten Island\"\n",
        "    return \"New York\"\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Optional enrichment: bring S3 borough back in (improves city field)\n",
        "# ---------------------------\n",
        "def enrich_s3_borough(harm: pd.DataFrame, s3_path: Optional[str]) -> pd.DataFrame:\n",
        "    if not s3_path:\n",
        "        harm[\"borough_hint\"] = pd.NA\n",
        "        return harm\n",
        "\n",
        "    try:\n",
        "        s3 = pd.read_excel(s3_path).reset_index(drop=True)\n",
        "    except Exception:\n",
        "        harm[\"borough_hint\"] = pd.NA\n",
        "        return harm\n",
        "\n",
        "    if \"source_record_id\" in harm.columns:\n",
        "        harm[\"source_record_id\"] = harm[\"source_record_id\"].astype(str)\n",
        "\n",
        "    s3_rows = harm.loc[harm[\"source_dataset\"] == \"S3\", \"source_record_id\"].dropna()\n",
        "    if s3_rows.empty:\n",
        "        harm[\"borough_hint\"] = pd.NA\n",
        "        return harm\n",
        "\n",
        "    width = int(s3_rows.astype(str).str.len().max())\n",
        "\n",
        "    s3[\"source_record_id\"] = [f\"{i+1:0{width}d}\" for i in range(len(s3))]\n",
        "    s3[\"source_record_id\"] = s3[\"source_record_id\"].astype(str)\n",
        "\n",
        "    aux = s3[[\"source_record_id\", \"Borough\"]].rename(columns={\"Borough\": \"borough_hint\"})\n",
        "    harm = harm.merge(aux, on=\"source_record_id\", how=\"left\")\n",
        "    return harm\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Census batch geocoding\n",
        "# ---------------------------\n",
        "def census_batch_geocode(batch_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if batch_df.empty:\n",
        "        return pd.DataFrame(\n",
        "            columns=[\n",
        "                \"id\",\n",
        "                \"census_match\",\n",
        "                \"census_match_type\",\n",
        "                \"census_matched_address\",\n",
        "                \"census_lon\",\n",
        "                \"census_lat\",\n",
        "                \"census_tigerline_id\",\n",
        "                \"census_side\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    out = io.StringIO()\n",
        "    writer = csv.writer(out, lineterminator=\"\\n\")\n",
        "    for _, r in batch_df.iterrows():\n",
        "        writer.writerow([r[\"id\"], r[\"street\"], r[\"city\"], r[\"state\"], r.get(\"zip\", \"\") or \"\"])\n",
        "    out.seek(0)\n",
        "\n",
        "    files = {\"addressFile\": (\"batch.csv\", out.getvalue(), \"text/csv\")}\n",
        "    params = {\"benchmark\": CENSUS_BENCHMARK, \"vintage\": CENSUS_VINTAGE}\n",
        "\n",
        "    resp = requests.post(CENSUS_BATCH_URL, params=params, files=files, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    rows = []\n",
        "    reader = csv.reader(io.StringIO(resp.text))\n",
        "    for row in reader:\n",
        "        if len(row) < 6:\n",
        "            continue\n",
        "\n",
        "        rid = row[0].strip()\n",
        "        match = row[2].strip() if len(row) > 2 else \"\"\n",
        "        match_type = row[3].strip() if len(row) > 3 else \"\"\n",
        "        matched_addr = row[4].strip() if len(row) > 4 else \"\"\n",
        "        coords = row[5].strip() if len(row) > 5 else \"\"\n",
        "\n",
        "        lon, lat = (np.nan, np.nan)\n",
        "        if coords and \",\" in coords:\n",
        "            parts = [p.strip() for p in coords.split(\",\")]\n",
        "            if len(parts) == 2:\n",
        "                try:\n",
        "                    lon = float(parts[0])\n",
        "                    lat = float(parts[1])\n",
        "                except Exception:\n",
        "                    lon, lat = (np.nan, np.nan)\n",
        "\n",
        "        tiger = row[6].strip() if len(row) > 6 else \"\"\n",
        "        side = row[7].strip() if len(row) > 7 else \"\"\n",
        "\n",
        "        rows.append(\n",
        "            {\n",
        "                \"id\": rid,\n",
        "                \"census_match\": match,\n",
        "                \"census_match_type\": match_type,\n",
        "                \"census_matched_address\": matched_addr,\n",
        "                \"census_lon\": lon,\n",
        "                \"census_lat\": lat,\n",
        "                \"census_tigerline_id\": tiger if tiger else pd.NA,\n",
        "                \"census_side\": side if side else pd.NA,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Run geocoding: add coords where missing, keep provenance, no merging\n",
        "# ---------------------------\n",
        "harm = pd.read_csv(\n",
        "    IN_HARMONIZED,\n",
        "    dtype={\"entry_id\": \"string\", \"source_dataset\": \"string\", \"source_record_id\": \"string\"},\n",
        ")\n",
        "\n",
        "if \"source_dataset\" not in harm.columns or harm[\"source_dataset\"].isna().all():\n",
        "    harm[\"source_dataset\"] = harm[\"entry_id\"].astype(str).str.split(\":\").str[0]\n",
        "else:\n",
        "    m = harm[\"source_dataset\"].isna()\n",
        "    harm.loc[m, \"source_dataset\"] = harm.loc[m, \"entry_id\"].astype(str).str.split(\":\").str[0]\n",
        "\n",
        "harm = enrich_s3_borough(harm, S3_PATH)\n",
        "\n",
        "harm[\"address_kind\"] = harm[\"address_raw\"].map(classify_address_kind)\n",
        "\n",
        "coords_missing = harm[\"lat\"].isna() | harm[\"lon\"].isna()\n",
        "\n",
        "harm[\"geocode_attempted\"] = False\n",
        "harm[\"geocode_skip_reason\"] = pd.NA\n",
        "\n",
        "harm.loc[coords_missing & harm[\"address_raw\"].isna(), \"geocode_skip_reason\"] = \"missing_address\"\n",
        "harm.loc[\n",
        "    coords_missing & harm[\"address_kind\"].isin([\"intersection\", \"between_phrase\", \"multi_address\", \"descriptive\"]),\n",
        "    \"geocode_skip_reason\",\n",
        "] = harm.loc[\n",
        "    coords_missing & harm[\"address_kind\"].isin([\"intersection\", \"between_phrase\", \"multi_address\", \"descriptive\"]),\n",
        "    \"address_kind\",\n",
        "]\n",
        "\n",
        "needs_geocode = coords_missing & harm[\"address_raw\"].notna()\n",
        "\n",
        "batch_rows = []\n",
        "for _, r in harm.loc[needs_geocode].iterrows():\n",
        "    kind = r.get(\"address_kind\")\n",
        "    if kind != \"street_address\":\n",
        "        continue\n",
        "\n",
        "    street = clean_street_for_geocode(r[\"address_raw\"])\n",
        "    if not street:\n",
        "        if pd.isna(harm.loc[harm[\"entry_id\"] == r[\"entry_id\"], \"geocode_skip_reason\"]).all():\n",
        "            harm.loc[harm[\"entry_id\"] == r[\"entry_id\"], \"geocode_skip_reason\"] = \"unusable_address_string\"\n",
        "        continue\n",
        "\n",
        "    borough = norm_ws(r.get(\"borough_hint\"))\n",
        "    city = borough_to_city(borough)\n",
        "    z = extract_zip(r[\"address_raw\"])\n",
        "\n",
        "    batch_rows.append(\n",
        "        {\n",
        "            \"id\": str(r[\"entry_id\"]),\n",
        "            \"street\": street,\n",
        "            \"city\": city,\n",
        "            \"state\": STATE_DEFAULT,\n",
        "            \"zip\": z or \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "batch_df = pd.DataFrame(batch_rows)\n",
        "\n",
        "if not batch_df.empty:\n",
        "    harm.loc[harm[\"entry_id\"].isin(batch_df[\"id\"].astype(str)), \"geocode_attempted\"] = True\n",
        "\n",
        "results = census_batch_geocode(batch_df)\n",
        "\n",
        "harm[\"geocode_provider\"] = pd.NA\n",
        "harm[\"geocode_status\"] = pd.NA\n",
        "harm[\"geocode_match_type\"] = pd.NA\n",
        "harm[\"geocode_matched_address\"] = pd.NA\n",
        "harm[\"geocode_query\"] = pd.NA\n",
        "\n",
        "res_map = results.set_index(\"id\").to_dict(orient=\"index\") if not results.empty else {}\n",
        "\n",
        "for i, r in harm.iterrows():\n",
        "    if not (pd.isna(r[\"lat\"]) or pd.isna(r[\"lon\"])):\n",
        "        continue\n",
        "\n",
        "    rid = str(r[\"entry_id\"])\n",
        "\n",
        "    if rid not in res_map:\n",
        "        if r.get(\"geocode_attempted\") is True and pd.isna(r.get(\"geocode_status\")):\n",
        "            harm.at[i, \"geocode_provider\"] = \"US Census Geocoder\"\n",
        "            harm.at[i, \"geocode_status\"] = \"No_Match\"\n",
        "        continue\n",
        "\n",
        "    rr = res_map[rid]\n",
        "    harm.at[i, \"geocode_provider\"] = \"US Census Geocoder\"\n",
        "    harm.at[i, \"geocode_status\"] = rr.get(\"census_match\", pd.NA)\n",
        "    harm.at[i, \"geocode_match_type\"] = rr.get(\"census_match_type\", pd.NA)\n",
        "    harm.at[i, \"geocode_matched_address\"] = rr.get(\"census_matched_address\", pd.NA)\n",
        "\n",
        "    qrow = batch_df.loc[batch_df[\"id\"] == rid]\n",
        "    if not qrow.empty:\n",
        "        harm.at[i, \"geocode_query\"] = (\n",
        "            f'{qrow.iloc[0][\"street\"]}, {qrow.iloc[0][\"city\"]}, {qrow.iloc[0][\"state\"]} {qrow.iloc[0][\"zip\"]}'.strip()\n",
        "        )\n",
        "\n",
        "    if str(rr.get(\"census_match\", \"\")).lower() == \"match\":\n",
        "        harm.at[i, \"lon\"] = rr.get(\"census_lon\", np.nan)\n",
        "        harm.at[i, \"lat\"] = rr.get(\"census_lat\", np.nan)\n",
        "        harm.at[i, \"space_basis\"] = \"geocoded_census\"\n",
        "        harm.at[i, \"spatial_precision\"] = \"point\"\n",
        "        harm.at[i, \"geocode_skip_reason\"] = pd.NA\n",
        "    else:\n",
        "        if harm.at[i, \"geocode_attempted\"] is True and pd.isna(harm.at[i, \"geocode_skip_reason\"]):\n",
        "            harm.at[i, \"geocode_skip_reason\"] = \"geocoder_returned_no_match\"\n",
        "\n",
        "# Post-pass: make attempted failures explicit for auditability (A method)\n",
        "mask_no_match = (\n",
        "    (harm[\"geocode_attempted\"] == True)\n",
        "    & (harm[\"geocode_status\"].astype(str).str.lower() == \"no_match\")\n",
        "    & (harm[\"geocode_skip_reason\"].isna())\n",
        ")\n",
        "harm.loc[mask_no_match, \"geocode_skip_reason\"] = \"geocoder_no_match\"\n",
        "\n",
        "harm.to_csv(OUT_GEOCODED, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"Wrote: {OUT_GEOCODED} (rows={len(harm)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GVPItGxFQIC4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}