{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code Snippet 1. Harmonization script"
      ],
      "metadata": {
        "id": "_9v65bS1KNhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import re\n",
        "from ast import literal_eval\n",
        "from typing import Any, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 0) File paths (project local)\n",
        "# ---------------------------\n",
        "S1_PATH = \"/content/S1_NYCLGBT Historic Sites Project.csv\"\n",
        "S2_PATH = \"/content/S2_Addresses project.csv\"\n",
        "S3_PATH = \"/content/S3_Gwendolyn Stegall data.xlsx\"\n",
        "\n",
        "OUT_HARMONIZED = \"harmonized_core.csv\"\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Minimal string helpers\n",
        "# ---------------------------\n",
        "def norm_ws(x: Any) -> Optional[str]:\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return None\n",
        "    s = str(x).strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s if s else None\n",
        "\n",
        "\n",
        "def canonicalize_name(name_raw: Optional[str]) -> Optional[str]:\n",
        "    if not name_raw:\n",
        "        return None\n",
        "    s = name_raw\n",
        "    s = s.replace(\"‐\", \"-\").replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
        "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s if s else None\n",
        "\n",
        "\n",
        "def parse_pylist_str(x: Any) -> List[Any]:\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return []\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    s = str(x).strip()\n",
        "    try:\n",
        "        out = literal_eval(s)\n",
        "        return out if isinstance(out, list) else []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Temporalization helpers\n",
        "# ---------------------------\n",
        "def decade_token_to_int(dec: str) -> Optional[int]:\n",
        "    if not dec:\n",
        "        return None\n",
        "    m = re.match(r\"^\\s*(\\d{4})s\\s*$\", dec)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "\n",
        "def normalize_decade_tokens(tokens: List[str]) -> Tuple[List[str], Optional[str]]:\n",
        "    clean: List[str] = []\n",
        "    audit: List[str] = []\n",
        "\n",
        "    for raw in tokens:\n",
        "        if not raw:\n",
        "            continue\n",
        "        t = str(raw).strip()\n",
        "        t = t.replace(\"‐\", \"-\").replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
        "\n",
        "        if \"?\" in t:\n",
        "            audit.append(f\"uncertain_decade:{t}\")\n",
        "            t = t.replace(\"?\", \"\").strip()\n",
        "\n",
        "        if re.match(r\"^\\d{4}s$\", t):\n",
        "            clean.append(t)\n",
        "        elif re.match(r\"^\\d{4}s-\\d{4}s$\", t):\n",
        "            a, b = t.split(\"-\")\n",
        "            ai, bi = decade_token_to_int(a), decade_token_to_int(b)\n",
        "            if ai is None or bi is None or ai > bi:\n",
        "                audit.append(f\"bad_decade_range:{t}\")\n",
        "            else:\n",
        "                for y in range(ai, bi + 1, 10):\n",
        "                    clean.append(f\"{y}s\")\n",
        "        else:\n",
        "            audit.append(f\"unparsed_decade:{t}\")\n",
        "\n",
        "    clean = sorted(set(clean), key=lambda d: decade_token_to_int(d) or 99999)\n",
        "    return clean, (\"; \".join(audit) if audit else None)\n",
        "\n",
        "\n",
        "def approx_start_end_from_decades(decades: List[str]) -> Tuple[Optional[int], Optional[int]]:\n",
        "    if not decades:\n",
        "        return None, None\n",
        "    ints = [decade_token_to_int(d) for d in decades]\n",
        "    ints = [i for i in ints if i is not None]\n",
        "    if not ints:\n",
        "        return None, None\n",
        "    return min(ints), max(ints) + 9\n",
        "\n",
        "\n",
        "def decades_from_free_text(text: Optional[str]) -> Tuple[List[str], Optional[int], Optional[int], Optional[str]]:\n",
        "    if not text:\n",
        "        return [], None, None, None\n",
        "\n",
        "    s = text.replace(\"‐\", \"-\").replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
        "    audit: List[str] = []\n",
        "\n",
        "    years = [int(y) for y in re.findall(r\"(?<!\\d)(18\\d{2}|19\\d{2}|20\\d{2})(?!\\d)\", s)]\n",
        "    start_year = min(years) if years else None\n",
        "    end_year = max(years) if years else None\n",
        "\n",
        "    dec_tokens = re.findall(r\"(?<!\\d)(18\\d0s|19\\d0s|20\\d0s)(?!\\d)\", s)\n",
        "    decades_norm, dec_audit = normalize_decade_tokens(dec_tokens)\n",
        "    if dec_audit:\n",
        "        audit.append(dec_audit)\n",
        "\n",
        "    if not decades_norm and years:\n",
        "        decades_norm = sorted(set(f\"{(y // 10) * 10}s\" for y in years), key=decade_token_to_int)\n",
        "\n",
        "    if re.search(r\"\\b(today|present|current|ongoing)\\b\", s, flags=re.I):\n",
        "        end_year = None\n",
        "        audit.append(\"relative_time_token\")\n",
        "\n",
        "    return decades_norm, start_year, end_year, (\"; \".join(audit) if audit else None)\n",
        "\n",
        "\n",
        "def to_json_list(x: List[str]) -> str:\n",
        "    return json.dumps(x, ensure_ascii=False)\n",
        "\n",
        "\n",
        "def enforce_schema_types(out: pd.DataFrame) -> pd.DataFrame:\n",
        "    out[\"lat\"] = pd.to_numeric(out[\"lat\"], errors=\"coerce\").astype(\"Float64\")\n",
        "    out[\"lon\"] = pd.to_numeric(out[\"lon\"], errors=\"coerce\").astype(\"Float64\")\n",
        "    out[\"start_year\"] = pd.to_numeric(out[\"start_year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    out[\"end_year\"] = pd.to_numeric(out[\"end_year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Source adapters → harmonized schema\n",
        "# ---------------------------\n",
        "HARMONIZED_COLS = [\n",
        "    \"entry_id\",\n",
        "    \"source_dataset\",\n",
        "    \"source_record_id\",\n",
        "    \"name_raw\",\n",
        "    \"name_canonical\",\n",
        "    \"site_type_tags\",\n",
        "    \"evidence_mode\",\n",
        "    \"primary_source\",\n",
        "    \"corroboration_status\",\n",
        "    \"corroborating_sources\",\n",
        "    \"active_raw\",\n",
        "    \"start_year\",\n",
        "    \"end_year\",\n",
        "    \"decades_norm\",\n",
        "    \"time_basis\",\n",
        "    \"address_raw\",\n",
        "    \"lat\",\n",
        "    \"lon\",\n",
        "    \"space_basis\",\n",
        "    \"spatial_precision\",\n",
        "    \"audit_note\",\n",
        "]\n",
        "\n",
        "def harmonize_s1(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out[\"source_dataset\"] = \"S1\"\n",
        "    out[\"source_record_id\"] = df[\"source_id\"].astype(str)\n",
        "    out[\"entry_id\"] = [f\"S1:{rid}\" for rid in out[\"source_record_id\"]]\n",
        "\n",
        "    out[\"name_raw\"] = df[\"name\"].map(norm_ws)\n",
        "    out[\"name_canonical\"] = out[\"name_raw\"].map(canonicalize_name)\n",
        "    out[\"address_raw\"] = df[\"address_label\"].map(norm_ws)\n",
        "\n",
        "    # Spatial\n",
        "    out[\"lat\"] = pd.to_numeric(df[\"latitude\"], errors=\"coerce\")\n",
        "    out[\"lon\"] = pd.to_numeric(df[\"longitude\"], errors=\"coerce\")\n",
        "    out[\"space_basis\"] = np.where(\n",
        "        out[\"lat\"].notna() & out[\"lon\"].notna(),\n",
        "        \"source_coordinates\",\n",
        "        \"address_only\"\n",
        "    )\n",
        "    out[\"spatial_precision\"] = np.where(out[\"space_basis\"].eq(\"source_coordinates\"), \"point\", None)\n",
        "\n",
        "    # Temporal (decade-level only; NO year derivation)\n",
        "    raw_dec = df[\"decades\"].map(parse_pylist_str)\n",
        "    dec_norm: List[List[str]] = []\n",
        "    audit: List[Optional[str]] = []\n",
        "\n",
        "    for toks in raw_dec:\n",
        "        clean, a = normalize_decade_tokens([str(t) for t in toks])\n",
        "        dec_norm.append(clean)\n",
        "        audit.append(a)\n",
        "\n",
        "    out[\"decades_norm\"] = [to_json_list(x) for x in dec_norm]\n",
        "\n",
        "    # Keep year fields empty for S1 (avoid implying continuous occupation)\n",
        "    out[\"start_year\"] = pd.Series([pd.NA] * len(out), dtype=\"Int64\")\n",
        "    out[\"end_year\"] = pd.Series([pd.NA] * len(out), dtype=\"Int64\")\n",
        "\n",
        "    # More explicit basis label (significance timeframe, decade-scale)\n",
        "    out[\"time_basis\"] = np.where(\n",
        "        pd.Series(dec_norm).map(len) > 0,\n",
        "        \"S1.decades_significance\",\n",
        "        None\n",
        "    )\n",
        "\n",
        "    out[\"audit_note\"] = audit\n",
        "\n",
        "    # Other harmonized fields\n",
        "    out[\"site_type_tags\"] = [to_json_list([]) for _ in range(len(out))]\n",
        "    out[\"evidence_mode\"] = \"institutional_inventory\"\n",
        "    out[\"primary_source\"] = None\n",
        "    out[\"active_raw\"] = None\n",
        "    out[\"corroboration_status\"] = \"single_source\"\n",
        "    out[\"corroborating_sources\"] = to_json_list([])\n",
        "\n",
        "    out = out.reindex(columns=HARMONIZED_COLS)\n",
        "    return enforce_schema_types(out)\n",
        "\n",
        "\n",
        "\n",
        "def harmonize_s2(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out[\"source_dataset\"] = \"S2\"\n",
        "\n",
        "    width = len(str(len(df)))\n",
        "    out[\"source_record_id\"] = [f\"{i+1:0{width}d}\" for i in range(len(df))]\n",
        "    out[\"entry_id\"] = [f\"S2:{rid}\" for rid in out[\"source_record_id\"]]\n",
        "\n",
        "    out[\"name_raw\"] = df[\"Name\"].map(norm_ws)\n",
        "    out[\"name_canonical\"] = out[\"name_raw\"].map(canonicalize_name)\n",
        "    out[\"address_raw\"] = df[\"Location\"].map(norm_ws)\n",
        "    out[\"active_raw\"] = df[\"Active\"].map(norm_ws)\n",
        "\n",
        "    out[\"lat\"] = pd.to_numeric(df.get(\"Latitude\"), errors=\"coerce\")\n",
        "    out[\"lon\"] = pd.to_numeric(df.get(\"Longitude\"), errors=\"coerce\")\n",
        "    out[\"space_basis\"] = np.where(out[\"lat\"].notna() & out[\"lon\"].notna(), \"source_coordinates\", \"address_only\")\n",
        "    out[\"spatial_precision\"] = np.where(out[\"space_basis\"].eq(\"source_coordinates\"), \"point\", None)\n",
        "\n",
        "    dec_norm: List[List[str]] = []\n",
        "    start_y: List[Optional[int]] = []\n",
        "    end_y: List[Optional[int]] = []\n",
        "    audit: List[Optional[str]] = []\n",
        "\n",
        "    for t in out[\"active_raw\"].tolist():\n",
        "        d, sy, ey, a = decades_from_free_text(t)\n",
        "        dec_norm.append(d)\n",
        "        start_y.append(sy)\n",
        "        end_y.append(ey)\n",
        "        audit.append(a)\n",
        "\n",
        "    out[\"decades_norm\"] = [to_json_list(x) for x in dec_norm]\n",
        "\n",
        "    approx = [approx_start_end_from_decades(x) for x in dec_norm]\n",
        "    out[\"start_year\"] = [sy if sy is not None else ap[0] for sy, ap in zip(start_y, approx)]\n",
        "    out[\"end_year\"] = [ey if ey is not None else ap[1] for ey, ap in zip(end_y, approx)]\n",
        "    out[\"time_basis\"] = np.where(out[\"active_raw\"].notna(), \"S2.active_raw\", None)\n",
        "    out[\"audit_note\"] = audit\n",
        "\n",
        "    out[\"site_type_tags\"] = [to_json_list([]) for _ in range(len(out))]\n",
        "    out[\"evidence_mode\"] = \"community_directory\"\n",
        "    out[\"primary_source\"] = None\n",
        "    out[\"corroboration_status\"] = \"single_source\"\n",
        "    out[\"corroborating_sources\"] = to_json_list([])\n",
        "\n",
        "    out = out.reindex(columns=HARMONIZED_COLS)\n",
        "    return enforce_schema_types(out)\n",
        "\n",
        "\n",
        "def harmonize_s3(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out[\"source_dataset\"] = \"S3\"\n",
        "\n",
        "    width = len(str(len(df)))\n",
        "    out[\"source_record_id\"] = [f\"{i+1:0{width}d}\" for i in range(len(df))]\n",
        "    out[\"entry_id\"] = [f\"S3:{rid}\" for rid in out[\"source_record_id\"]]\n",
        "\n",
        "    out[\"name_raw\"] = df[\"Bar Name\"].map(norm_ws)\n",
        "    out[\"name_canonical\"] = out[\"name_raw\"].map(canonicalize_name)\n",
        "    out[\"address_raw\"] = df[\"Address\"].map(norm_ws)\n",
        "\n",
        "    def map_tags(cat: Any) -> List[str]:\n",
        "        c = (norm_ws(cat) or \"\").lower()\n",
        "        tags: List[str] = []\n",
        "        if \"bar\" in c or \"club\" in c:\n",
        "            tags.append(\"bar_club\")\n",
        "        if \"after hours\" in c:\n",
        "            tags.append(\"after_hours\")\n",
        "        if \"party\" in c or \"night\" in c or \"disco\" in c:\n",
        "            tags.append(\"nightlife_event\")\n",
        "        return sorted(set(tags))\n",
        "\n",
        "    out[\"site_type_tags\"] = df[\"Category\"].map(map_tags).map(to_json_list)\n",
        "\n",
        "    open_y = pd.to_numeric(df[\"Open\"], errors=\"coerce\")\n",
        "    closed_y = pd.to_numeric(df[\"Closed\"], errors=\"coerce\")\n",
        "    out[\"start_year\"] = open_y.where(open_y.notna(), None)\n",
        "    out[\"end_year\"] = closed_y.where(closed_y.notna(), None)\n",
        "\n",
        "    dec_field = df[\"Decade(s)\"].map(norm_ws)\n",
        "    dec_norm: List[List[str]] = []\n",
        "    audit: List[Optional[str]] = []\n",
        "\n",
        "    for x in dec_field.tolist():\n",
        "        if not x:\n",
        "            dec_norm.append([])\n",
        "            audit.append(None)\n",
        "            continue\n",
        "        toks = re.split(r\"[;,]| and | & \", x)\n",
        "        toks = [t.strip() for t in toks if t and t.strip()]\n",
        "        clean, a = normalize_decade_tokens(toks)\n",
        "        dec_norm.append(clean)\n",
        "        audit.append(a)\n",
        "\n",
        "    out[\"decades_norm\"] = [to_json_list(x) for x in dec_norm]\n",
        "\n",
        "    approx = [approx_start_end_from_decades(x) for x in dec_norm]\n",
        "    out[\"start_year\"] = [\n",
        "        int(sy) if sy is not None and not (isinstance(sy, float) and np.isnan(sy)) else ap[0]\n",
        "        for sy, ap in zip(out[\"start_year\"].tolist(), approx)\n",
        "    ]\n",
        "    out[\"end_year\"] = [\n",
        "        int(ey) if ey is not None and not (isinstance(ey, float) and np.isnan(ey)) else ap[1]\n",
        "        for ey, ap in zip(out[\"end_year\"].tolist(), approx)\n",
        "    ]\n",
        "\n",
        "    out[\"time_basis\"] = np.where(df[\"Open\"].notna() | df[\"Closed\"].notna(), \"S3.open_closed\", \"S3.decades\")\n",
        "    out[\"audit_note\"] = audit\n",
        "\n",
        "    out[\"lat\"] = pd.Series([pd.NA] * len(out), dtype=\"Float64\")\n",
        "    out[\"lon\"] = pd.Series([pd.NA] * len(out), dtype=\"Float64\")\n",
        "    out[\"space_basis\"] = \"address_only\"\n",
        "    out[\"spatial_precision\"] = None\n",
        "\n",
        "    out[\"evidence_mode\"] = \"scholarly_reconstruction\"\n",
        "    out[\"primary_source\"] = df.get(\"Primary Source\", pd.Series([None] * len(df))).map(norm_ws)\n",
        "    out[\"corroboration_status\"] = \"single_source\"\n",
        "    out[\"corroborating_sources\"] = to_json_list([])\n",
        "    out[\"active_raw\"] = None\n",
        "\n",
        "    out = out.reindex(columns=HARMONIZED_COLS)\n",
        "    return enforce_schema_types(out)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Run harmonization (no cross-source merging here)\n",
        "# ---------------------------\n",
        "s1 = pd.read_csv(S1_PATH)\n",
        "s2 = pd.read_csv(S2_PATH)\n",
        "s3 = pd.read_excel(S3_PATH)\n",
        "\n",
        "h1 = harmonize_s1(s1)\n",
        "h2 = harmonize_s2(s2)\n",
        "h3 = harmonize_s3(s3)\n",
        "\n",
        "harm = pd.concat([h1, h2, h3], ignore_index=True)\n",
        "\n",
        "assert harm[\"entry_id\"].is_unique, \"entry_id must be unique at this stage\"\n",
        "assert list(harm.columns) == HARMONIZED_COLS, \"harmonized schema mismatch\"\n",
        "\n",
        "harm.to_csv(OUT_HARMONIZED, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"Wrote: {OUT_HARMONIZED} (rows={len(harm)})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnCmf2QuKP32",
        "outputId": "8e1bef96-d3bf-4727-97bc-88921f0da245"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: harmonized_core.csv (rows=542)\n"
          ]
        }
      ]
    }
  ]
}