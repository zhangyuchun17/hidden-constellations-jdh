# -*- coding: utf-8 -*-
"""JDH-Code Snippet 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IR7v_PBMT_y3cClOGRZVueA7HLoqwTAk

# Code Snippet 1. Harmonization script
"""

from __future__ import annotations

import json
import pandas as pd
import numpy as np
import re
from typing import Any, Optional


# ============================================================
# Code Snippet 1
# Source ingestion and schema alignment (no cross-source merge)
# ============================================================

# ---------------------------
# Inputs
# ---------------------------
S1_PATH = "/content/S1_NYCLGBT Historic Sites Project.csv"
S2_PATH = "/content/S2_Addresses Project.csv"
S3_PATH = "/content/S3_Gwendolyn Stegall data.xlsx"

# ---------------------------
# Output
# ---------------------------
OUT_HARMONIZED = "harmonized_core.csv"


# ---------------------------
# Helpers (minimal, non-interpretive)
# ---------------------------
def norm_ws(x: Any) -> Optional[str]:
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return None
    s = str(x).strip()
    return s if s else None


def enforce_types(df: pd.DataFrame) -> pd.DataFrame:
    df["lat"] = pd.to_numeric(df["lat"], errors="coerce").astype("Float64")
    df["lon"] = pd.to_numeric(df["lon"], errors="coerce").astype("Float64")
    df["start_year"] = pd.to_numeric(df["start_year"], errors="coerce").astype("Int64")
    df["end_year"] = pd.to_numeric(df["end_year"], errors="coerce").astype("Int64")
    return df


def parse_decades_from_text(x: Any) -> list[str]:
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return []
    years = [int(y) for y in re.findall(r"\d{4}", str(x))]
    if not years:
        return []
    start, end = min(years), max(years)
    return [f"{d}s" for d in range((start // 10) * 10, (end // 10) * 10 + 1, 10)]


def parse_decades_from_bounds(start_year, end_year) -> list[str]:
    years = []
    if pd.notna(start_year):
        years.append(int(start_year))
    if pd.notna(end_year):
        years.append(int(end_year))
    if not years:
        return []
    start, end = min(years), max(years)
    return [f"{d}s" for d in range((start // 10) * 10, (end // 10) * 10 + 1, 10)]


def finalize_decades_and_basis(
    reported_decades: pd.Series,
    derived_decades: pd.Series,
    source_tag: str
) -> tuple[pd.Series, pd.Series]:
    """
    Unified rule across S1 / S2 / S3:
    - reported if parseable decade text exists
    - derived if not reported but derivable from year bounds
    - missing otherwise
    """

    decades_norm = np.where(
        reported_decades.map(len) > 0,
        reported_decades.map(json.dumps),
        np.where(
            derived_decades.map(len) > 0,
            derived_decades.map(json.dumps),
            json.dumps([])
        )
    )

    time_basis = np.select(
        [
            reported_decades.map(len) > 0,
            (reported_decades.map(len) == 0) & (derived_decades.map(len) > 0)
        ],
        [
            f"{source_tag}.decades_reported",
            f"{source_tag}.decades_derived_from_bounds"
        ],
        default=f"{source_tag}.decades_missing"
    )

    return decades_norm, time_basis


# ---------------------------
# Harmonized schema
# ---------------------------
HARMONIZED_COLS = [
    "entry_id",
    "source_dataset",
    "source_record_id",
    "name_raw",
    "name_canonical",
    "site_type_tags",
    "evidence_mode",
    "primary_source",
    "corroboration_status",
    "corroborating_sources",
    "active_raw",
    "start_year",
    "end_year",
    "decades_norm",
    "time_basis",
    "address_raw",
    "lat",
    "lon",
    "space_basis",
    "spatial_precision",
    "audit_note",
]


# ---------------------------
# S1 — NYCLGBT Historic Sites Project
# ---------------------------
def ingest_s1(df: pd.DataFrame) -> pd.DataFrame:
    out = pd.DataFrame(index=df.index)

    out["source_dataset"] = "S1"
    out["source_record_id"] = df["source_id"].astype(str)
    out["entry_id"] = "S1:" + out["source_record_id"]

    out["name_raw"] = df["name"].map(norm_ws)
    out["name_canonical"] = out["name_raw"]
    out["address_raw"] = df["address_label"].map(norm_ws)

    out["lat"] = pd.to_numeric(df["latitude"], errors="coerce")
    out["lon"] = pd.to_numeric(df["longitude"], errors="coerce")
    out["space_basis"] = np.where(out["lat"].notna(), "source_coordinates", "address_only")
    out["spatial_precision"] = np.where(out["lat"].notna(), "point", None)

    out["start_year"] = pd.NA
    out["end_year"] = pd.NA

    reported = df["decades"].apply(parse_decades_from_text)
    derived = pd.Series([[]] * len(df), index=df.index)

    out["decades_norm"], out["time_basis"] = finalize_decades_and_basis(
        reported, derived, "S1"
    )

    out["site_type_tags"] = json.dumps([])
    out["evidence_mode"] = "institutional_inventory"
    out["primary_source"] = None
    out["active_raw"] = None
    out["corroboration_status"] = "single_source"
    out["corroborating_sources"] = json.dumps([])
    out["audit_note"] = None

    return enforce_types(out[HARMONIZED_COLS])


# ---------------------------
# S2 — Addresses Project
# ---------------------------
def ingest_s2(df: pd.DataFrame) -> pd.DataFrame:
    out = pd.DataFrame(index=df.index)

    out["source_dataset"] = "S2"
    out["source_record_id"] = df.index.astype(str)
    out["entry_id"] = "S2:" + out["source_record_id"]

    out["name_raw"] = df["Name"].map(norm_ws)
    out["name_canonical"] = out["name_raw"]
    out["address_raw"] = df["Location"].map(norm_ws)

    out["lat"] = pd.to_numeric(df["Latitude"], errors="coerce")
    out["lon"] = pd.to_numeric(df["Longitude"], errors="coerce")
    out["space_basis"] = "source_coordinates"
    out["spatial_precision"] = "point"

    out["start_year"] = df["Start_Year"]
    out["end_year"] = df["End_Year"]

    reported = df["Decades"].apply(parse_decades_from_text)
    derived = out.apply(
        lambda r: parse_decades_from_bounds(r["start_year"], r["end_year"]),
        axis=1
    )

    out["decades_norm"], out["time_basis"] = finalize_decades_and_basis(
        reported, derived, "S2"
    )

    out["site_type_tags"] = json.dumps([])
    out["evidence_mode"] = "community_directory"
    out["primary_source"] = None
    out["active_raw"] = None
    out["corroboration_status"] = "single_source"
    out["corroborating_sources"] = json.dumps([])
    out["audit_note"] = None

    return enforce_types(out[HARMONIZED_COLS])


# ---------------------------
# S3 — Stegall (2019) reconstruction
# ---------------------------
def ingest_s3(df: pd.DataFrame) -> pd.DataFrame:
    out = pd.DataFrame(index=df.index)

    out["source_dataset"] = "S3"
    out["source_record_id"] = df.index.astype(str)
    out["entry_id"] = "S3:" + out["source_record_id"]

    out["name_raw"] = df["Bar Name"].map(norm_ws)
    out["name_canonical"] = out["name_raw"]
    out["address_raw"] = df["Address"].map(norm_ws)

    out["lat"] = pd.NA
    out["lon"] = pd.NA
    out["space_basis"] = "address_only"
    out["spatial_precision"] = None

    out["start_year"] = pd.to_numeric(df["Open"], errors="coerce")
    out["end_year"] = pd.to_numeric(df["Closed"], errors="coerce")

    reported = df["Decade(s)"].apply(parse_decades_from_text)
    derived = out.apply(
        lambda r: parse_decades_from_bounds(r["start_year"], r["end_year"]),
        axis=1
    )

    out["decades_norm"], out["time_basis"] = finalize_decades_and_basis(
        reported, derived, "S3"
    )

    out["site_type_tags"] = json.dumps([])
    out["evidence_mode"] = "scholarly_reconstruction"
    out["primary_source"] = df.get("Primary Source")
    out["active_raw"] = None
    out["corroboration_status"] = "single_source"
    out["corroborating_sources"] = json.dumps([])
    out["audit_note"] = None

    return enforce_types(out[HARMONIZED_COLS])


# ---------------------------
# Run ingestion
# ---------------------------
s1 = pd.read_csv(S1_PATH)
s2 = pd.read_csv(S2_PATH)
s3 = pd.read_excel(S3_PATH)

h1 = ingest_s1(s1)
h2 = ingest_s2(s2)
h3 = ingest_s3(s3)

harmonized = pd.concat([h1, h2, h3], ignore_index=True)
harmonized.to_csv(OUT_HARMONIZED, index=False, encoding="utf-8-sig")

print(f"Wrote {OUT_HARMONIZED} ({len(harmonized)} rows)")